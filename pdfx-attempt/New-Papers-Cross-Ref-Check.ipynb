{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfx in /anaconda3/lib/python3.6/site-packages (1.3.0)\n",
      "Requirement already satisfied: tika in /anaconda3/lib/python3.6/site-packages (1.19)\n",
      "Requirement already satisfied: chardet in /anaconda3/lib/python3.6/site-packages (from pdfx) (3.0.4)\n",
      "Requirement already satisfied: pdfminer2 in /anaconda3/lib/python3.6/site-packages (from pdfx) (20151206)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.6/site-packages (from tika) (2.18.4)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from tika) (38.4.0)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from pdfminer2->pdfx) (1.11.0)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests->tika) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests->tika) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests->tika) (2018.1.18)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "import json\n",
    "!{sys.executable} -m pip install pdfx tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfx\n",
    "paperdirectory = \"/Users/fernandodurier/Desktop/sarcasm-paper-review\"\n",
    "import os\n",
    "set_of_refs = []\n",
    "for root, dirs, files in os.walk(paperdirectory, topdown=False):\n",
    "    for name in files:\n",
    "        if \".pdf\" in name:\n",
    "            pdfpath = os.path.join(root, name)\n",
    "            pdf = pdfx.PDFx(pdfpath)\n",
    "            metadata = pdf.get_metadata()\n",
    "            references_list = pdf.get_references()\n",
    "            references_dict = pdf.get_references_as_dict()\n",
    "            pdfmeta = {\"meta\":metadata, \"references\":references_dict}\n",
    "            set_of_refs.append(pdfmeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['AuthoritativeDomain[1]', 'AuthoritativeDomain[2]', 'CreationDate', 'CrossMarkDomains[1]', 'CrossMarkDomains[2]', 'CrossmarkDomainExclusive', 'CrossmarkMajorVersionDate', 'ElsevierWebPDFSpecifications', 'ModDate', 'Producer', 'doi', 'robots', 'xap', 'pdf', 'dc', 'xapmm', 'http://prismstandard.org/namespaces/basic/3.0/', 'pdfx', 'http://www.niso.org/schemas/ali/1.0/', 'http://www.niso.org/schemas/jav/1.0/', 'crossmark', 'rights', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['Author', 'CreationDate', 'Creator', 'Keywords', 'ModDate', 'PTEX.Fullbanner', 'Producer', 'Title', 'Trapped', 'dc', 'xap', 'pdf', 'pdfx', 'xapmm', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['CreationDate', 'Creator', 'ModDate', 'PTEX.Fullbanner', 'Producer', 'Trapped', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['Author', 'Title', 'Subject', 'Creator', 'Producer', 'CreationDate', 'ModDate', 'Trapped', 'PTEX.Fullbanner', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['Producer', 'Creator', 'CreationDate', 'ModDate', 'Trapped', 'PTEX.Fullbanner', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['CreationDate', 'Creator', 'ModDate', 'PTEX.Fullbanner', 'Producer', 'Trapped', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['Author', 'CreationDate', 'Creator', 'CrossMarkDomains[1]', 'CrossMarkDomains[2]', 'CrossmarkDomainExclusive', 'CrossmarkMajorVersionDate', 'ElsevierWebPDFSpecifications', 'ModDate', 'Producer', 'Subject', 'Title', 'doi', 'robots', 'dc', 'http://prismstandard.org/namespaces/basic/3.0/', 'crossmark', 'http://www.niso.org/schemas/jav/1.0/', 'pdfx', 'xap', 'rights', 'pdf', 'xapmm', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['Producer', 'Creator', 'CreationDate', 'ModDate', 'Trapped', 'PTEX.Fullbanner', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['CrossMarkDomains[2]', 'CreationDate', 'CrossmarkMajorVersionDate', 'Subject', 'Author', 'Creator', 'Keywords', 'ElsevierWebPDFSpecifications', 'CrossmarkDomainExclusive', 'robots', 'ModDate', 'doi', 'CrossMarkDomains[1]', 'Title', 'dc', 'http://prismstandard.org/namespaces/basic/3.0/', 'crossmark', 'pdfx', 'http://www.niso.org/schemas/jav/1.0/', 'xap', 'rights', 'pdf', 'xapmm', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['IEEE Issue ID', 'Creator', 'Producer', 'IEEE Article ID', 'Title', 'IEEE Publication ID', 'Meeting Ending Date', 'Meeting Starting Date', 'Subject', 'ModDate', 'CreationDate', 'dc', 'http://prismstandard.org/namespaces/basic/3.0/', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['AuthoritativeDomain[1]', 'AuthoritativeDomain[2]', 'CreationDate', 'CrossMarkDomains[1]', 'CrossMarkDomains[2]', 'CrossmarkDomainExclusive', 'CrossmarkMajorVersionDate', 'ElsevierWebPDFSpecifications', 'ModDate', 'Producer', 'doi', 'robots', 'xap', 'pdf', 'dc', 'xapmm', 'http://prismstandard.org/namespaces/basic/3.0/', 'pdfx', 'http://www.niso.org/schemas/ali/1.0/', 'http://www.niso.org/schemas/jav/1.0/', 'crossmark', 'rights', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['CreationDate', 'Creator', 'ModDate', 'Producer', 'Title', 'xap', 'pdf', 'dc', 'xapmm', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['PTEX.Fullbanner', 'IEEE Issue ID', 'Creator', 'Producer', 'IEEE Article ID', 'Title', 'IEEE Publication ID', 'Subject', 'ModDate', 'Trapped', 'CreationDate', 'dc', 'http://prismstandard.org/namespaces/basic/3.0/', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['CrossMarkDomains[2]', 'CreationDate', 'CrossmarkMajorVersionDate', 'Subject', 'Author', 'Creator', 'Keywords', 'ElsevierWebPDFSpecifications', 'CrossmarkDomainExclusive', 'robots', 'ModDate', 'doi', 'CrossMarkDomains[1]', 'Title', 'dc', 'http://prismstandard.org/namespaces/basic/3.0/', 'crossmark', 'pdfx', 'http://www.niso.org/schemas/jav/1.0/', 'xap', 'rights', 'pdf', 'xapmm', 'http://www.niso.org/schemas/ali/1.0/', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['CreationDate', 'Creator', 'ModDate', 'PTEX.Fullbanner', 'Producer', 'Trapped', 'xap', 'pdfx', 'pdf', 'dc', 'xapmm', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['ModDate', 'CreationDate', 'Title', 'Creator', 'Producer', 'Pages'])\n",
      "-------------------------------------------------------------\n",
      "meta_keys:  dict_keys(['IEEE Issue ID', 'Creator', 'Producer', 'IEEE Article ID', 'Title', 'IEEE Publication ID', 'Meeting Ending Date', 'Meeting Starting Date', 'Subject', 'ModDate', 'CreationDate', 'dc', 'http://prismstandard.org/namespaces/basic/3.0/', 'Pages'])\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "for ref in set_of_refs:\n",
    "#     print('meta: ', ref['meta'])\n",
    "#     print('references: ', ref['references'])\n",
    "    print('meta_keys: ', ref['meta'].keys())\n",
    "    print('-------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "paperdirectory = \"/Users/fernandodurier/Desktop/sarcasm-paper-review\"\n",
    "import os\n",
    "import tika\n",
    "from tika import parser\n",
    "set_of_refs_tika = []\n",
    "for root, dirs, files in os.walk(paperdirectory, topdown=False):\n",
    "    for name in files:\n",
    "        if \".pdf\" in name:\n",
    "            pdfpath = os.path.join(root, name)\n",
    "            parsed = parser.from_file(pdfpath)\n",
    "            pdfmeta = parsed['metadata']\n",
    "            set_of_refs_tika.append(pdfmeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "keywords:  Fine-Grained Opinion Mining; Ideology Detection; Sarcasm Detection \n",
      "-------------------------------------------------------------\n",
      "keywords:  \n",
      "-------------------------------------------------------------\n",
      "keywords:  \n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "keywords:  \n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "keywords:  Irony detection; Text mining; Twitter; Politics\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "keywords:  \n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "keywords:  Big data; Flume; Hadoop; Hive; MapReduce; Sarcasm; Sentiment; Tweets\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "for ref in set_of_refs_tika:\n",
    "#    print(\"keys:\", ref.keys())\n",
    "    \n",
    "    if 'Keywords' in ref.keys():\n",
    "        print('keywords: ', ref['Keywords'])\n",
    "        \n",
    "    elif 'meta:keyword' in ref.keys():\n",
    "        print('keywords: ', ref['meta:keyword'])\n",
    "        \n",
    "    elif 'dc:subject' in ref.keys():\n",
    "        print('keywords: ', ref['dc:subject']) \n",
    "    \n",
    "    print('-------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for root, dirs, files in os.walk(paperdirectory, topdown=False):\n",
    "    for name in files:\n",
    "        if \"-deep.json\" in name:\n",
    "            with open(paperdirectory+\"/\"+name) as f:\n",
    "                d = json.load(f)\n",
    "                data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "authors:  ['Martin Flintham', 'Christian Karner', 'Khaled Bachour', 'Helen Creswick', 'Neha Gupta', 'Stuart Moran']\n",
      "title:  Falling for Fake News: Investigating the Consumption of News via Social Media\n",
      "pub_date:  {'year': '2018'}\n",
      "keywords:  None\n",
      "------------------------------------------------------------\n",
      "authors:  [{'@contrib-type': 'author', 'email': 'e.kapogianni@kent.ac.uk', 'string-name': 'Eleni Kapogianni', 'xref': {'#text': '0', '@ref-type': 'aff', '@rid': 'aff0'}}]\n",
      "title:  The ironic operation: Revisiting the components of ironic meaning\n",
      "pub_date:  {'year': '2015'}\n",
      "keywords:  ['Irony', 'Ironic operation', 'Ironic vehicle', '(Direct) ironic output']\n",
      "------------------------------------------------------------\n",
      "authors:  ['Kai Shuy', 'Amy Slivaz', 'Suhang Wangy', 'Jiliang Tang \\\\', 'Huan Liuy']\n",
      "title:  Fake News Detection on Social Media: A Data Mining Perspective\n",
      "pub_date:  None\n",
      "keywords:  None\n",
      "------------------------------------------------------------\n",
      "authors:  ['Dubai', 'United Arab Emirates', 'c a Jihen Karoui', 'Farah BSanoa', 'rla MZit', 'VeÂ´ronique Moriceau']\n",
      "title:  SOUKHRIA: Towards an Irony Detection System for Arabic in 2017, Dubai, United Arab Emirates Social Media SOUKHRIA: Towards an Irony Detection System for Arabic in\n",
      "pub_date:  {'year': '2017'}\n",
      "keywords:  ['Arabic tweets', 'opinion analysis', 'irony detection', 'supervised learning']\n",
      "------------------------------------------------------------\n",
      "authors:  ['Manjusha P D', 'M. Tech Student', 'QL manjushapda@gmail.com']\n",
      "title:  Convolutional Neural Network based Simile Classification System\n",
      "pub_date:  {'year': '2018'}\n",
      "keywords:  None\n",
      "------------------------------------------------------------\n",
      "authors:  ['H M Keerthi Kumar', 'B S Harish', 'H M Keerthi Kumar', 'B S Harish']\n",
      "title:  Sarcasm classification: A novel approach by using Content Based Sarcasm classification: A novel approach by using Content Based Sarcasm classification: A novel approach by using Content Based Feature Selection Method Feature Selection Method Feature Selection Method\n",
      "pub_date:  {'year': '2018'}\n",
      "keywords:  ['Sarcasm', 'Classification', 'Feature Selection', 'k-means clustering']\n",
      "------------------------------------------------------------\n",
      "authors:  ['LE HOANG SON', 'AKSHI KUMAR', 'SAURABH RAJ SANGWAN', 'ANSHIKA ARORA', 'ANAND NAYYAR', 'MOHAMED ABDEL-BASSET']\n",
      "title:  Sarcasm Detection Using Soft Attention-Based Bidirectional Long Short-Term Memory Model With Convolution Network\n",
      "pub_date:  {'year': '2019'}\n",
      "keywords:  ['Sarcasm', 'deep learning', 'attention', 'social data']\n",
      "------------------------------------------------------------\n",
      "authors:  ['Rossano Schifanella', 'Paloma de Juan', 'Liangliang Cao', 'Joel Tetreault', 'Sarcasm; Social Media; Multimodal; Deep Learning; NLP']\n",
      "title:  Detecting Sarcasm in Multimodal Social Platforms\n",
      "pub_date:  None\n",
      "keywords:  None\n",
      "------------------------------------------------------------\n",
      "authors:  [{'@contrib-type': 'author', 'email': 'nwenwemdy08@gmail.com', 'string-name': 'Nwe Nwe', 'xref': {'#text': '0', '@ref-type': 'aff', '@rid': 'aff0'}}]\n",
      "title:  None\n",
      "pub_date:  {'year': '2017'}\n",
      "keywords:  None\n",
      "------------------------------------------------------------\n",
      "authors:  ['S.K. Bharti n', 'B. Vachha', 'R.K. Pradhan', 'K.S. Babu', 'S.K. Jena']\n",
      "title:  Sarcastic sentiment detection in tweets streamed in real time: a big data approach\n",
      "pub_date:  {'year': '2016'}\n",
      "keywords:  ['Big data', 'Flume', 'Hadoop', 'Hive', 'MapReduce', 'Sarcasm', 'Sentiment', 'Tweets']\n",
      "------------------------------------------------------------\n",
      "authors:  [None]\n",
      "title:  Fake News or Truth? Using Satirical Cues to Detect Potentially Misleading News.\n",
      "pub_date:  {'year': '2016'}\n",
      "keywords:  None\n",
      "------------------------------------------------------------\n",
      "authors:  ['Giuseppe Castellucci', 'Danilo Croce', 'Roberto Basili']\n",
      "title:  Context-aware Convolutional Neural Networks for Twitter Sentiment Analysis in Italian\n",
      "pub_date:  None\n",
      "keywords:  None\n",
      "------------------------------------------------------------\n",
      "authors:  ['Fan Yang and Arjun Mukherjee', 'Eduard Gragut']\n",
      "title:  Satirical News Detection and Analysis using Attention Mechanism and Linguistic Features\n",
      "pub_date:  None\n",
      "keywords:  None\n",
      "------------------------------------------------------------\n",
      "authors:  [{'@contrib-type': 'author', 'email': 's.kannangara@unsw.edu.au', 'string-name': 'Sandeepa Kannangara', 'xref': {'#text': '0', '@ref-type': 'aff', '@rid': 'aff0'}}]\n",
      "title:  Mining Twiter for Fine-Grained Political Opinion Polarity Classification, Ideology Detection and Sarcasm Detection\n",
      "pub_date:  {'year': '2018'}\n",
      "keywords:  None\n",
      "------------------------------------------------------------\n",
      "authors:  ['Basilis Charalampakis', 'Dimitris Spathis n', 'Elias Kouslis', 'Katia Kermanidis']\n",
      "title:  A comparison between semi-supervised and supervised text mining techniques on detecting irony in greek political tweets\n",
      "pub_date:  {'year': '2016'}\n",
      "keywords:  ['Irony detection', 'Text mining', 'Twitter', 'Politics']\n",
      "------------------------------------------------------------\n",
      "authors:  ['Rafael A. Monteiro', 'Roney L. S. Santos', 'Thiago A. S. Pardo', 'Tiago A. de Almeida', 'Evandro E. S. Ruiz', 'Oto A. Vale']\n",
      "title:  Contributions to the Study of Fake News in Portuguese: New Corpus and Automatic Detection Results\n",
      "pub_date:  None\n",
      "keywords:  ['Fake news', 'Reference corpus', 'Linguistic features', 'Machine learning']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------------')\n",
    "for d in data:\n",
    "    print(\"authors: \", d['clean-authors'])\n",
    "    print(\"title: \", d['title'])\n",
    "    print(\"pub_date: \", d['pub_date'])\n",
    "    print(\"keywords: \", d['keywords'])\n",
    "    print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
