<?xml version="1.0" encoding="UTF-8"?>
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta />
    <article-meta>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Nwe Nwe</string-name>
          <email>nwenwemdy08@gmail.com</email>
          <xref ref-type="aff" rid="aff0">0</xref>
        </contrib>
        <aff id="aff0">
          <label>0</label>
          <institution>University of Computer Studies</institution>
          ,
          <addr-line>Mandalay Mandalay</addr-line>
          ,
          <country country="MM">Myanmar</country>
        </aff>
      </contrib-group>
      <pub-date>
        <year>2017</year>
      </pub-date>
      <fpage>24</fpage>
      <lpage>26</lpage>
      <abstract>
        <p>- Due to the implicit traits embedded in tweets, handling figurative languages appear as the most trending topics in computational linguistics. While recognition of a single language is hard to capture, differentiating several languages at once is the most challenging task. To achieve this purpose, we employ a set of emotion-based features in order to individuate between humor, irony, sarcasm, satire and true. We use eight basic emotions excerpted from EmoLex supplement with tweets polarity. We apply these features in two datasets: balanced dataset (collected using hashtag-based approach) and class-imbalanced dataset (collected from streaming tweets). As a result, the model not only outperform a word-based baseline but also handle both balanced and class-imbalanced datasets in multi-figurative language detection.</p>
      </abstract>
    </article-meta>
  </front>
  <body>
    <sec id="sec-1">
      <title>-</title>
      <p>Keywords?Figurative Language; Humor; Irony; Sarcasm;
Satire; Emotion Detection; Class-imbalanced problem</p>
    </sec>
    <sec id="sec-2">
      <title>I. INTRODUCTION</title>
      <p>Apart from literal language, figurative language is a peculiar
form of communication which use words deviate from its actual
meanings. It takes multiple forms, such as similes, metaphor,
irony, sarcasm, satire etc. This kind of languages can be
extensively found in various outlets: literature, television, the
internet, social media, comics and cartoons. Due to its
widespread usage, handling figurative language is currently one
of the most challenging tasks in computational linguistics, natural
language processing and social multimedia sentiment analysis.</p>
      <p>
        Some researchers tried to recognize the sentiments embedded
in figurative languages [7] while some emphasized on the
recognition of one or more figurative languages [
        <xref ref-type="bibr" rid="ref10">1, 2, 9, 12</xref>
        ].
Whereas, some contributed new trending topics in figurative
language analysis: satire recognition [
        <xref ref-type="bibr" rid="ref11 ref9">11, 13</xref>
        ], and fake news
detection [
        <xref ref-type="bibr" rid="ref19">21</xref>
        ]. Most of the existing works used intrinsic features
such as lexical signatures, temporal and contextual signatures etc.
However, due to the implicit nature of the language, there is no
work that try to tackle several figurative languages at once from
emotional point of view.
      </p>
      <p>Moreover, most of the existing efforts worked upon the
balanced datasets which have nearly equal number of instances in
each class [2, 11-13, and 16]. But, thanks to the nature of web,
not all tweets are figurative. Even though they are widely used
communication forms, their involvement in each day stream is
less than the majority literal language. As shown in Table 1, we
collected the 2-day stream tweets about Hillary Clinton
(?#hillary?) after U.S. presidential election date. We can see that
the majority of the stream is true tweets and the minority is
figurative languages (e.g. 8376:3 and 8376:2).
1
3</p>
      <p>Sarcasm</p>
      <p>Satire</p>
      <p>Humor
1
2
0
0
0
0</p>
      <p>True
6544
8376</p>
      <p>Total
6546
8381</p>
      <p>Implementing a classifier on such imbalanced data provides
misleading classification accuracy called ?Accuracy Paradox?. It
means that we may have the excellent accuracy (such as 90%
accuracy), but it?s the reflecting of underlying majority class
distribution. So, handling class-imbalanced problem is also one
of the greatest issues in data mining. Based on these issues, we
contributed as follows:
?
?
?</p>
      <p>The evaluation of our satire detection model (Pyae Phyo
Thu and Nwe Nwe, 2017) to multi-figurative language
recognition
A set of experiments to demonstrate that our
multifigurative language recognition framework can achieve
on both balanced and class-imbalanced datasets
A set of Twitter datasets for multi-figurative language
recognition in both balanced and class-imbalanced
configurations
The rest of the paper is constructed as follows. We first discuss
related works in Section II. Then, the data preprocessing and
tools are presented in Section III. Features used in the model is
explained in Section IV. The results of the experiments are
discussed in Section V. And then, we conclude the paper and
point to its future directions in Section VI.</p>
    </sec>
    <sec id="sec-3">
      <title>II. RELATED WORK</title>
      <p>Due to the advent used in internet and social multimedia,
handling figurative languages is well-studied phenomena in
computational linguistics, psychology, cognitive science and
social multimedia analysis. In summary, the techniques used in
language analysis can be categorized as follows:</p>
      <sec id="sec-3-1">
        <title>A. Signature Analysis</title>
        <p>This feature is focused on exploring specific textual markers
or signatures embedded in the language. Kreuz and Caucci
(2007) investigated the influences of lexical features such as
presence of adjectives, verbs, interjection and punctuation on
sarcasm detection. Veale and Hao (2010) looked at framing
device: ?as GROUND as a VEHICLE? to separate ironic from
nonironic similes. In 2011, Gonzalez-Ibanez et al. investigated the
impact of lexical and pragmatic factors such as unigram features,
dictionary-based features, positive emoticons, negative emoticons
and ToUser for identifying the sarcastic utterances. In 2013,
Reyes et al. constructed a new model of irony detection that can
assess along with four types of conceptual features: signatures,
unexpectedness, style and emotional scenarios. Even though the
signature analysis is a form of foundational studies, it give the
promising results to differentiate between figurative languages.</p>
      </sec>
      <sec id="sec-3-2">
        <title>B. Frequency Analysis</title>
        <p>
          In order to independent from the lexical and semantic factors,
Francesco Barbieri et al. proposed a language-independent
intrinsic features for multi-lingual recognition of Irony [
          <xref ref-type="bibr" rid="ref8">10</xref>
          ],
Sarcasm [
          <xref ref-type="bibr" rid="ref12">14</xref>
          ], Humor [
          <xref ref-type="bibr" rid="ref10">12</xref>
          ] and Satire [
          <xref ref-type="bibr" rid="ref11 ref9">11, 13</xref>
          ]. A group of
frequency based features that measures the frequency of rare
words, ambiguity, POS tag, synonyms, sentiments and characters
has proposed. The performance of the model is demonstrated by
executing experiments on both monolingual and cross-language
classifications: English, Spanish, and Italian. Nevertheless,
Barbieri also pointed to the difficulty of figurative language
recognition from emotional outlook due to ironic dimension [
          <xref ref-type="bibr" rid="ref11">13</xref>
          ].
        </p>
      </sec>
      <sec id="sec-3-3">
        <title>C. Contextual Analysis</title>
        <p>The most recent form of analysis used in computational
linguistics is the contextual analysis. The term contextualization
means any features of linguistic that contributes to the signaling
of contextual presuppositions. Wallace et al. (2014) provided the
first empirical evidence about the important of context to make
judgments on ironic intent. In 2015, Wallace et al. further
exploited a set of contextual features for irony detection by
combining noun phrases and sentiment extracted from reddit
comments. Joshi et al. (2015) also exploited two kinds of context
incongruity features: explicit incongruity such as ?I love being
ignored? and implicit incongruity such as ?I love this paper so
much that I made a doggy bag out of it?. These two features are
implemented on the two text forms ? tweets and discussion
forum posts. The system outperformed two past works in sarcasm
detection with F-score improvement of 10-20%. Khattri et al
(2015) presented the first quantitative evidence to show that
historical tweets by an author can provide additional context for
sarcasm detection by using (1) a contrast-based predictor - to
identify whether there is a sentiment contrast within a target
tweet and (2) a historical tweet-based predictor - to identify if the
sentiment expressed towards an entity agrees with author past
sentiment expressed towards that entity. Bamman and Smith
(2015) also showed that the extra-linguistic information such as
properties of author, the audience and the immediate
communicate environment can lead to the improvements in
sarcasm prediction accuracy.</p>
      </sec>
      <sec id="sec-3-4">
        <title>D. Supervised/Semi supervised Techniques</title>
        <p>Another form of techniques used in figurative language
analysis is the use of classification algorithm in language
detection. Tsur et al. (2010) presented a novel semi-supervised
recognition of sarcastic sentences in product reviews. In order to
recognize the sarcastic sentences, the model first extracted
patterns in semi-supervised form and then classified the sarcastic
utterances using that features. The model achieved precision of
77% and recall of 83%. Buschmeier et al. (2014) analyzed the
impact of a number of features such as imbalance, hyperbole,
quotes, punctuation, interjections using several different
classification approaches: Linear SVM, Logistic Regression,
Decision Tree, Random Forest and Na畏e Bayes. Fersini et al.
(2015) introduced the Bayesian Model Averaging (BMA)
approach employed on pragmatic particles and POS tags in order
to distinguish between the irony and sarcasm in microblogs. The
results showed that BMA outperformed the traditional state of art
model and also able to ensure notable generalization capabilities
on ironic and sarcastic text.</p>
        <p>In this paper, we use the emotion-based signatures such as
basic emotions, Bag-of Sorted Emotion (BOSE) and its unigram,
bigram, trigram TFIDF scores. In order to achieve the better
performance, these features are implemented on balanced and
class-imbalanced datasets bundled with Ensemble Bagging
classifier.</p>
      </sec>
    </sec>
    <sec id="sec-4">
      <title>III. TEXT PROCESSING AND TOOLS</title>
      <sec id="sec-4-1">
        <title>A. Tools Usage</title>
        <p>We used two different kinds of tools: one for tweets
acquisition (Tweepy) and one for score computing (S테NCE).</p>
        <p>1) Tweepy: To be able to accumulate twitter data, an easy
to use python library, Tweepy (http://www.tweepy.org), is used
to access Twitter API.</p>
        <p>2) Sentiment analysis and social cognition engine
(S테NCE): It is a freely available tool for sentiment, emotion,
and social cognition analysis [6]. It allows the batch processing
of text files with built in sentimental and emotional databases:
GALC, EmoLex, ANEW, SENTIC, VADER, Hu-Liu, GI and
Lasswell. Apart from the original databases, it offer the wide
range of components scores with or without negation. In this
study, S테NCE is used to compute the score of eight basic
emotions (EmoLex) and score of sentiment polarity (VADER).</p>
      </sec>
      <sec id="sec-4-2">
        <title>B. Datasets</title>
        <p>One of the purpose of this study is to examine whether the
multi-figurative model can implement on both balanced and
class-imbalanced datasets. So, we collected two different kinds of
twitter datasets: (1) traditional balanced dataset and (2) streaming
datasets. Although there are two approaches for class annotation:
inter-annotators agreement and the author self-defined hashtag
approach, we used the hashtag-based annotation to label whether
the tweet is ironic/sarcasm/ satire/humor or true.</p>
        <p>1) Balanced Dataset: In order to form the balanced
datasets, we collected about 5000 tweets with hashtags ?#irony?,
?#sarcasm?, ?#satire?, ?#humor? with no date or person
limitations. We also collected about 5000 tweets which have no
figurative language hashtag. After cleaning and removing
inconvenient tweets, about 4000 tweets for each language
(around 20000 tweets in total) are collected as shown in Table II.</p>
        <p>2) Class-imbalanced Dataset: For class-imbalanced
dataset, we utilized the streaming tweets. However, instead of
collecting a whole day stream, we collected the streaming tweets
with hashtag ?#hillary?. Because of being debatable period
between two politicians and their supporters, ?Hillary? and
?Trump? tweets composed of several figurative languages with
user-defined hashtag label #satire, #irony, #sarcasm, #humor.
However, not all a-day stream tweets contain all figurative
languages. So, we randomly picked a-day stream tweets for
Hillary that composed of all figurative languages. As shown in
Table II, we used November 29th streaming tweets for Hillary
which contained all figurative languages.</p>
        <p>Corpus
Balanced
Dataset
ClassImbalanced
Dataset</p>
        <p>The task of figurative language detection is modelled as a
multi-class supervised classification problem to differentiate the
incoming tweets as humor/ironic/sarcastic/satirical or true tweets.
We implement Ensemble Bagging classifier compared with
standard Linear SVM classifier and focus on the impact analysis
of different features by investigating their effects on the system
performance. Features and classifier that will use in model are as
follows:</p>
      </sec>
      <sec id="sec-4-3">
        <title>A. Features</title>
        <p>We analyze the tweets based on five major indices:
1) Word-based: This simple representation of bag-of word is
used as the text classification benchmark. The highest 1000 most
frequent occurrences of word n-gram model (unigram/bigram/
trigram) are selected as features and filter out the rest.</p>
        <p>2) Emotion: Eight basic EmoLex emotions: anger,
anticipation, disgust, fear, joy, sadness, surprise, trust and their
polarity valence computed by S테NCE are used as the basic
emotional features.</p>
        <p>3) Sentiment: VADER sentiment polarity: positive, negative,
neutral and compound sentiment scores excerpted from text
analysis tool S테NCE are used. VADER is a sentiment analysis
corpus that surpassed the prior corpus: LIWC, ANEW, GI,
SentiWordNet and Hu-Liu04 in social media sentiment analysis
[6]. Hence, supplementing the basic emotional features with
sentiment polarity is an essential to achieve better performance.
4) Bag-of Sorted Emotion (BOSE): We supplement the
basic emotional and sentimental features with Bag-of Sorted
Emotion (BOSE). It?s assembled from eight basic emotional
features and three sentimental features. As shown in Fig. 1, we
first sort the emotional features and sentiment features
individually. Our hypothesis is that figurative language will differ
each other in intended emotions. So, we determine the most
intended emotion and sentiment of the tweets. As a result, we
obtain a bag-of sorted emotion which contain a bag of intended
emotion, sorted emotions, intended sentiment and sorted
sentiments.
5) BOSE-TFIDF: Term frequency-inverse document
frequency (TFIDF) is an important weighting scheme extensively
used in information retrieval and text processing. We implement
TF-IDF weighting scheme on unigram, bigram, trigram BOSE
with class and sentiment consideration. For instance, based on the
result from Fig. 1, we obtain emoUnigram (Anticipation),
emoBigram (Anticipation-Fear), and emoTrigram
(AnticipationFear-Surprise). And then, TF-IDF score for each emoUnigram,
emoBigram and emoTrigram is computed. As a result, we
obtained BOSE-TFIDF score for each tweet.</p>
      </sec>
      <sec id="sec-4-4">
        <title>B. Classifiers</title>
        <p>We analyze the tweets with two classifiers:
1) SVM: We use Linear Support Vector Machine (SVM),
a discriminative classifier formally defined by a separating hyper
plane, as a benchmark classifier. We compare the results of SVM
classifier with the Ensemble Bagging classifier.</p>
        <p>2) Ensemble Bagging Classifier: To achieve the high
performance and more resilience to noise, the model use
Ensemble Bagging classifier. Bootstrap Aggregation (Bagging) is
an ensemble generation method that uses variations of samples to
train base classifiers. It selects N samples from the training set
with size N and train a base classifier. This is repeated until the
desired size of the ensemble is reached. We employ bagging
classifier on our datasets: balanced datasets and class-imbalanced
datasets.</p>
      </sec>
    </sec>
    <sec id="sec-5">
      <title>V. EXPERIMENTAL RESULTS AND DISCUSSION In order to test the performance of the model, we run 2-fold and 5-fold cross validation on both balanced and class</title>
      <p>imbalanced datasets. The model run upon Windows 10 (64-bit)
PC with Intel Core i3 processor, 8GB RAM and 1GB graphic
card.</p>
      <sec id="sec-5-1">
        <title>A. Experiments on Balanced Dataset</title>
        <p>Table III report the F1-score of each features employed on
balanced dataset. Table III (a) demonstrated the outcomes of
SVM and Table III (b) demonstrated the outcomes of Ensemble
Bagging classifier.</p>
        <p>In SVM classification, whether 2-Fold or 5-Fold, it showed
that the model left behind the baseline: BoW (about 0.43 versus
0.77/ 0.78). It is obvious that ?Sentiment? is the best among the
features with 0.34/0.33 score. The others: Emotion (0.27/0.30),
BOSE (0.33/0.32), and BOSE-TFIDF (0.15) did not perform as
well as Sentiment. In BoW, Sentiment and BOSE, ?Irony? is the
best categorized class with 0.88, 0.37/0.36 and 0.44/0.43 score
respectively. But, ?Sarcasm? is the best recognized in Emotion
and ?Satire? is the best recognized in BOSE_TFIDF. While we
combine all features, we observe that it can classify well in
?Sarcasm? (0.65) followed by ?Irony? (0.48).</p>
        <p>In Ensemble classification, feature combination offer the best
score 1.00 whereas baseline BoW only offer the score 0.78/0.79.
Like SVM, BoW, Sentiment and BOSE features best classified in
the class ?Irony? and the others: Emotion best classified the class
?Sarcasm?. It?s obvious that the majority of the highest score
come from BOSE_TFIDF. In conclusion, it?s obvious that the
model bundled with Ensemble Bagging classifier outperforms the
benchmark classifier: SVM on balanced dataset whether it?s in
2Fold or 5-Fold configurations.</p>
        <p>Table IV reported the F1-score of each features employed
on class-imbalanced dataset. Like balanced dataset, Table IV
(a) represented the results of benchmark: SVM and Table IV
(b) represented the results of Ensemble Bagging classifier.
However, unlike the balanced dataset, the testing set does not
cover all classes because almost all samples are already
involved in training and there is no samples left for testing. As
shown in Table IV (a) and (b), ?-? represented the class that do
not involve in testing. Even using 5-fold cross validation left so
many uncovered classes. So, we didn?t employ standard
10fold configuration in the experiments. Instead, 5-fold and
2fold configurations are used.</p>
        <p>In all cases: 2-fold/5-fold or SVM/Ensemble, it can be seen
that all give the highest score 1.00. However, it?s due to the
distribution of majority class: ?True?. It provides misleading
classification accuracy called ?Accuracy Paradox?. Moreover,
there are other classes that uniformly give the 0.00 score. In
SVM, almost all features except BOSE-TFIDF and ?all
features combination? did not classify their related classes
(give 0.00 score). However, BOSE-TFIDF can clearly classify
?Satire? with 0.57 score in 2-Fold cross validation and feature
combination can recognize ?Humor? and ?Satire? with 0.67
and 0.80 score respectively. At the same time, the benchmark:
BoW can?t classify any language except the majority ?True?
class. So, we can assume that the model outperform the
baseline: BoW in class-imbalanced SVM classification.</p>
        <p>In Table IV (b), benchmark: BoW can?t classify any classes
except ?Humor? (1.0) and ?Satire? (0.29). Other features:
Emotion, Sentiment and BOSE can?t capture any classes. It
only give worst score 0.00 in all cases. However,
BOSETFIDF offer better classification than others (?Irony-1? and
?Satire-0.80?). The combination of all features: Emotion,
Sentiment, BOSE and BOSE-TFIDF give the reliable
classification in both configurations compared with other
features (Humor-0.67, Satire-0.80, True-1.00, and Mean-1.00).</p>
        <p>In conclusion, it?s clear that the model bundled with
Ensemble classifier offer the best performance. In balanced
data, model give the 100% full performance score: 1.00. In
class-imbalanced data whether it?s 2-fold or 5-fold, our model
not only fix the class-imbalanced problem but also give the
best and reliable performance score: 1.00.</p>
      </sec>
      <sec id="sec-5-2">
        <title>C. Reliability Measurement using Cohen?s Kappa</title>
        <p>Determining the performance of the model using F-score is
not enough and it?s ambiguous. To clarify the performance of
the model, we measure the reliability of the model using
Cohen?s Kappa. It?s a statistic which measure the inter-rater
agreement for qualitative terms. It offers the classification
accuracy normalized by the imbalance of the classes in the
data. As shown in Table V and Fig. 2 and Fig. 3, in balanced
dataset, baseline: BoW offer the higher reliability (0.715) with
SVM classifier, the model offer the higher reliability (0.999)
with Ensemble Bagging classifier. In class-imbalanced
dataset, the model give the higher reliability than the BoW in
all cases: 2-Fold/5-Fold, SVM/Ensemble. However, the model
only offer the reliable measurement (0.799) in 2-Fold. In
5Fold configuration, it only give the score (0.499).
True
1.00
1.00
1.00
1.00
1.00
1.00
True
1.00
1.00
1.00
1.00
1.00
1.00
(b)
5-Fold
TABLE V.</p>
        <p>RELIABILITY MEASUREMENT STATISTICS USING COHEN?S KAPPA
Balanced Dataset</p>
        <p>Ensemble</p>
        <p>SVM</p>
        <p>Class-Imbalanced Dataset</p>
        <p>Ensemble
2-Fold
0.00
0.00
0.00
0.00
Fig. 2.</p>
      </sec>
    </sec>
    <sec id="sec-6">
      <title>RELIABILITY MEASUREMENT OF EACH FEATURES (2-FOLD CONFIGURATION) Fig. 3.</title>
    </sec>
    <sec id="sec-7">
      <title>RELIABILITY MEASUREMENT OF EACH FEATURES (5-FOLD CONFIGURATION)</title>
    </sec>
    <sec id="sec-8">
      <title>VI. CONCLUSION AND FUTURE WORK</title>
      <p>In this paper, we proposed the model to detect several
figurative languages: Humor, Irony, Satire, Sarcasm and
True. Even though ironic dimension of language lead
difficulty in differentiating each language, the model
successfully differentiate multi-figurative languages at once
from emotional point of view. Our model can employ on
both balanced and class-imbalanced datasets. Moreover, the
model significantly outperforms the baseline: BoW and
benchmark classifier: SVM. In future, we will implement the
model to recognize the fake news in social media.</p>
      <p>Byron C Wallace, Do Kook Choe, and Eugene Charniak, ?Sparse,
Contextually Informed Models for Irony Detection: Exploiting User
Communities, Entities and Sentiment?, Proceedings of the 53rd
Annual Meeting of the Association for Computational Linguistics and
the 7th International Joint Conference on Natural Language
Processing, pp. 1035?1044</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref id="ref1">
        <mixed-citation>
          <string-name>
            <given-names>Aditya</given-names>
            <surname>Joshi</surname>
          </string-name>
          , Vinita Sharma, and Pushpak Bhattacharyya, ?
          <article-title>Harnessing context incongruity for sarcasm detection?</article-title>
          ,
          <source>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</source>
          ,
          <year>2015</year>
          , Vol.
          <volume>2</volume>
          . pp.
          <fpage>757</fpage>
          -
          <lpage>762</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref2">
        <mixed-citation>
          <string-name>
            <given-names>Antonio</given-names>
            <surname>Reyes</surname>
          </string-name>
          , Paolo Rosso, and Tony Veale, ?
          <article-title>A multidimensional approach for detecting irony in twitter?, Language Resources and Evaluation (</article-title>
          <year>2013</year>
          ) 47: pp.
          <fpage>239</fpage>
          -
          <lpage>268</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref3">
        <mixed-citation>
          <string-name>
            <given-names>Anupam</given-names>
            <surname>Khattri</surname>
          </string-name>
          , Aditya Joshi, Pushpak Bhattacharyya, and Mark James Carman, ?
          <article-title>Your Sentiment Precedes You: Using an authors historical tweets to predict sarcasm?</article-title>
          ,
          <source>Proceedings of the 6th Workshop on Computational Approaches</source>
          to Subjectivity,
          <article-title>Sentiment and Social Media Analysis (WASSA</article-title>
          <year>2015</year>
          ), pp.
          <fpage>25</fpage>
          -
          <lpage>30</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref4">
        <mixed-citation>
          <string-name>
            <surname>Byron C Wallace</surname>
          </string-name>
          , Laura Kertz, Do Kook Choe, and Eugene Charniak, ?
          <article-title>Humans require context to infer ironic intent (so computers probably do, too)?</article-title>
          ,
          <source>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)</source>
          ,
          <year>2014</year>
          , pp.
        </mixed-citation>
      </ref>
      <ref id="ref5">
        <mixed-citation>
          <string-name>
            <surname>Crossley</surname>
            ,
            <given-names>S. A.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Kyle</surname>
            ,
            <given-names>K.</given-names>
          </string-name>
          , and
          <string-name>
            <surname>McNamara</surname>
            ,
            <given-names>D. S</given-names>
          </string-name>
          , ?
          <article-title>Sentiment analysis and social cognition engine (SEANCE): An automatic tool for sentiment, social cognition, and social order analysis?</article-title>
          ,
          <string-name>
            <given-names>D.S.</given-names>
            <surname>Behav Res</surname>
          </string-name>
          (
          <year>2016</year>
          ), doi:10.3758/s13428-016-0743-z Cynthia Van Hee,
          <article-title>Els Lefever and Veronique hoste, ?LT3: Sentiment Analysis of Figurative Tweets: piece of cake #NotReally?</article-title>
          ,
          <source>Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval</source>
          <year>2015</year>
          ), pp.
          <fpage>684</fpage>
          -
          <lpage>688</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref6">
        <mixed-citation>
          <string-name>
            <given-names>David</given-names>
            <surname>Bamman</surname>
          </string-name>
          and
          <article-title>Noah A Smith, ?Contextualized Sarcasm Detection on Twitter?</article-title>
          ,
          <string-name>
            <surname>In</surname>
            <given-names>ICWSM</given-names>
          </string-name>
          ,
          <year>2015</year>
          , pp.
          <fpage>574</fpage>
          -
          <lpage>577</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref7">
        <mixed-citation>
          <string-name>
            <given-names>Elisabetta</given-names>
            <surname>Fersini</surname>
          </string-name>
          , Federico Alberto Pozzi, Enza Messina, ?
          <article-title>Detecting irony and sarcasm in microblogs: The role of expressive signals and ensemble classifiers?</article-title>
          ,
          <source>In Data Science and Advanced Analytics (DSAA)</source>
          ,
          <year>2015</year>
          . IEEE International Conference on 2015 Oct 19, pp.
          <fpage>1</fpage>
          -
          <lpage>8</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref8">
        <mixed-citation>
          [10]
          <string-name>
            <surname>Francesco</surname>
            <given-names>Barbieri</given-names>
          </string-name>
          , Francesco Ronzano, and Horacio Saggion, ?
          <article-title>Italian irony detection in twitter: a first approach?</article-title>
          ,
          <source>In The First Italian Conference on Computational Linguistics CLiC-it 2014 &amp; the Fourth International Workshop EVALITA</source>
          ,
          <year>2014</year>
          , pp.
          <fpage>28</fpage>
          -
          <lpage>32</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref9">
        <mixed-citation>
          [11]
          <string-name>
            <surname>Francesco</surname>
            <given-names>Barbieri</given-names>
          </string-name>
          , Horacio Saggion, and Francesco Ronzano, ?
          <article-title>Is This Tweet Satirical? A computational Approach for Satire Detection in Spanish?</article-title>
          ,
          <source>Procesamiento del Lenguaje Natural</source>
          ,
          <year>2015</year>
          , Volume
          <volume>55</volume>
          , pp.
          <fpage>135</fpage>
          -
          <lpage>142</lpage>
        </mixed-citation>
      </ref>
      <ref id="ref10">
        <mixed-citation>
          [12]
          <string-name>
            <given-names>Francesco</given-names>
            <surname>Barbieri</surname>
          </string-name>
          and Horacio Saggion, ?
          <article-title>Automatic Detection of Irony and Humour in Twitter?</article-title>
          ,
          <source>In Proceedings of the International Conference on Computational Creativity</source>
          .
          <year>2014</year>
          .
        </mixed-citation>
      </ref>
      <ref id="ref11">
        <mixed-citation>
          [13]
          <string-name>
            <surname>Francesco</surname>
            <given-names>Barbieri</given-names>
          </string-name>
          , Horacio Saggion, and Francesco Ronzano, ?
          <article-title>Do We Criticise (and Laugh) in the same Way? Automatic Detection of Multi-Lingual Satirical News in Twitter?</article-title>
          ,
          <source>Proceedings of the TwentyFourth International Joint Conference on Artifical Intelligence (IJCAI</source>
          <year>2015</year>
          ), pp.
          <fpage>1215</fpage>
          -
          <lpage>1221</lpage>
        </mixed-citation>
      </ref>
      <ref id="ref12">
        <mixed-citation>
          [14]
          <string-name>
            <surname>Francesco</surname>
            <given-names>Barbieri</given-names>
          </string-name>
          , Horacio Saggion, and Francesco Ronzano, ?
          <article-title>Modelling Sarcasm in Twitter, a Novel Approach?</article-title>
          ,
          <source>Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</source>
          ,
          <year>2014</year>
          , pp.
          <fpage>50</fpage>
          -
          <lpage>58</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref13">
        <mixed-citation>
          [15]
          <string-name>
            <surname>Konstantin</surname>
            <given-names>Buschmeier</given-names>
          </string-name>
          , Philipp Cimiano, and Roman Klinger, ?
          <article-title>An impact analysis of features in a classification approach to irony detection in product reviews?</article-title>
          ,
          <source>In Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</source>
          ,
          <year>2014</year>
          , pp.
          <fpage>42</fpage>
          -
          <lpage>49</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref14">
        <mixed-citation>
          [16]
          <string-name>
            <given-names>Oren</given-names>
            <surname>Tsur</surname>
          </string-name>
          ,
          <string-name>
            <given-names>Dmitry</given-names>
            <surname>Davidov</surname>
          </string-name>
          , and Ari Rappoport, ?
          <string-name>
            <surname>ICWSM-A Great Catchy</surname>
          </string-name>
          <article-title>Name: Semi-Supervised Recognition of Sarcastic Sentences in Online Product Reviews?</article-title>
          ,
          <source>In ICWSM</source>
          <year>2010</year>
          , pp.
          <fpage>162</fpage>
          -
          <lpage>169</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref15">
        <mixed-citation>
          [17]
          <string-name>
            <given-names>Pyae</given-names>
            <surname>Phyo</surname>
          </string-name>
          Thu and Nwe Nwe, ?Understanding Social Media Satirical Emotion?,
          <year>2017</year>
          , in press
        </mixed-citation>
      </ref>
      <ref id="ref16">
        <mixed-citation>
          [18]
          <string-name>
            <surname>Roger J Kreuz and Gina M Caucci</surname>
          </string-name>
          , ?
          <article-title>Lexical influences on the perception of sarcasm?</article-title>
          .
          <source>In Proceedings of the Workshop on computational approaches to Figurative Language. Association for Computational Linguistics</source>
          ,
          <year>2007</year>
          , pp.
          <fpage>1</fpage>
          -
          <lpage>4</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref17">
        <mixed-citation>
          [19]
          <string-name>
            <given-names>Roberto</given-names>
            <surname>Gonz</surname>
          </string-name>
          <article-title>아lez-Ib아nez, Smaranda Muresan</article-title>
          , and NinaWacholder, ?
          <article-title>Identifying sarcasm in Twitter: a closer look?</article-title>
          ,
          <source>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume</source>
          <volume>2</volume>
          ,
          <year>2011</year>
          , pp.
          <fpage>581</fpage>
          -
          <lpage>586</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref18">
        <mixed-citation>
          [20]
          <string-name>
            <given-names>Tony</given-names>
            <surname>Veale</surname>
          </string-name>
          and Yanfen Hao, ?
          <article-title>Detecting Ironic Intent in Creative Comparisons?</article-title>
          ,
          <string-name>
            <surname>In</surname>
            <given-names>ECAI</given-names>
          </string-name>
          ,
          <year>2010</year>
          , Vol.
          <volume>215</volume>
          . pp.
          <fpage>765</fpage>
          -
          <lpage>770</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref19">
        <mixed-citation>
          [21]
          <string-name>
            <surname>Victoria</surname>
            <given-names>L.</given-names>
          </string-name>
          <string-name>
            <surname>Rubin</surname>
            ,
            <given-names>Niall J.</given-names>
          </string-name>
          <string-name>
            <surname>Conroy</surname>
          </string-name>
          , Yimin Chen, and Sarah Cornwell, ?
          <article-title>Fake News or Truth? Using Satirical Cues to Detect Potentially Misleading News?</article-title>
          ,
          <source>Proceedings of NAACL-HLT</source>
          <year>2016</year>
          , pp.
          <fpage>7</fpage>
          -
          <lpage>17</lpage>
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>

